<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Backup on Rockhopper.dk</title>
    <link>http://rockhopper.dk/categories/backup/</link>
    <description>Recent content in Backup on Rockhopper.dk</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 May 2016 14:08:31 +0200</lastBuildDate>
    <atom:link href="http://rockhopper.dk/categories/backup/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Duplicity backup: Dropbox vs Google drive back end</title>
      <link>http://rockhopper.dk/linux/software/duplicity-backup-dropbox-vs-google-drive-back-end</link>
      <pubDate>Mon, 02 May 2016 14:08:31 +0200</pubDate>
      
      <guid>http://rockhopper.dk/linux/software/duplicity-backup-dropbox-vs-google-drive-back-end</guid>
      <description>&lt;p&gt;As many other I have been looking for a cheap encrypted offsite backup for family photos, documents and other important data. I quickly choose duplicity as it provides the encrypted part and the possibility to use many different storage solutions, but the cheap offsite part is was a bit harder to find.&lt;/p&gt;

&lt;p&gt;I looked at Amazon offerings: S3 and Glacier, both look cheap, but I&#39;m unsure on how much I have to pay in total, and Glacier has crazy download prices.&lt;/p&gt;

&lt;p&gt;My second choice was Dropbox, and I purchased a Dropbox Pro account (1Tb) for EUR 99/year (~ DDK 744/year), but I canceled it after ~8 days, as upload was to slow (Thankfully I could get a refund). It took about ~50 hours to upload ~350Gb. Duplicity uses the &lt;a href=&#34;https://github.com/dropbox/dropbox-sdk-python/&#34;&gt;dropbox-python-sdk&lt;/a&gt;, and Dropbox API&#39;s method for uploading files larger than 150Mb is to use &amp;quot;chunked&amp;quot; upload; The first API call creates a session, and then data is append to this session until the entire file is uploaded. The chunk size can be configured, but 150Mb is the max size. So I experimented with different sizes and found that the default size of 16Mb would not utilize the bandwidth very good. But if I changed it to 150Mb I would see ~50Mbps for the entire file, but then would come a delay of 20-30secs where there was no network traffic to Dropbox:&lt;/p&gt;

&lt;div class=&#34;row image-gallery&#34;&gt;
  
&lt;div class=&#34;col-xs-6 col-sm-4 col-md-4&#34;&gt;
  &lt;a href=&#34;http://rockhopper.dk/uploads/duplicity_wan_upload_dropbox_150mb_chunks.png&#34;
    title=&#34;Upload to Dropbox with 150Mb chunk size&#34;
    class=&#34;image-link thumbnail&#34;
    
&gt;
    
        
        &lt;img src=&#34;http://rockhopper.dk/uploads/thumbnails/duplicity_wan_upload_dropbox_150mb_chunks-176x176.png&#34;
            class=&#34;
                &#34;
            
            
            
            
        /&gt;
    
    
    &lt;div class=&#34;caption&#34;&gt;Upload to Dropbox with 150Mb chunk size&lt;/div&gt;
&lt;/a&gt;

&lt;/div&gt;

&lt;div class=&#34;col-xs-6 col-sm-4 col-md-4&#34;&gt;
  &lt;a href=&#34;http://rockhopper.dk/uploads/duplicity_wan_upload_dropbox_16mb_chunks.png&#34;
    title=&#34;Upload to Dropbox with 16Mb chunk size&#34;
    class=&#34;image-link thumbnail&#34;
    
&gt;
    
        
        &lt;img src=&#34;http://rockhopper.dk/uploads/thumbnails/duplicity_wan_upload_dropbox_16mb_chunks-176x176.png&#34;
            class=&#34;
                &#34;
            
            
            
            
        /&gt;
    
    
    &lt;div class=&#34;caption&#34;&gt;Upload to Dropbox with 16Mb chunk size&lt;/div&gt;
&lt;/a&gt;

&lt;/div&gt;


&lt;/div&gt;


&lt;p&gt;I tested the chunk size by changing the DPBX_UPLOAD_CHUNK_SIZE in &lt;code&gt;/usr/lib/python2.7/site-packages/duplicity/backends/dpbxbackend.py&lt;/code&gt;
:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# This is chunk size for upload using Dpbx chumked API v2. It doesn&#39;t
# make sense to make it much large since Dpbx SDK uses connection pool
# internally. So multiple chunks will sent using same keep-alive socket
# Plus in case of network problems we most likely will be able to retry
# only failed chunk
DPBX_UPLOAD_CHUNK_SIZE = 16 * 1024 * 1024&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;So I decided to try Google Drive as back end, which was just fixed in duplicity version 0.7.07.1. Google drive utilized the bandwidth much better, and the delay between uploads is when duplicity prepares the next volume for upload.&lt;/p&gt;

&lt;p&gt;

&lt;div class=&#34;row&#34;&gt;
  &lt;div class=&#34;col-sm-4&#34;&gt;
    &lt;a href=&#34;http://rockhopper.dk/uploads/duplicity_wan_upload_google_drive_volsize_512mb.png&#34;
    title=&#34;Upload to Google Drive with 512Mb volsize&#34;
    class=&#34;image-link thumbnail&#34;
    
&gt;
    
        
        &lt;img src=&#34;http://rockhopper.dk/uploads/thumbnails/duplicity_wan_upload_google_drive_volsize_512mb-176x176.png&#34;
            class=&#34;
                &#34;
            
            
            
            
        /&gt;
    
    
    &lt;div class=&#34;caption&#34;&gt;Upload to Google Drive with 512Mb volsize&lt;/div&gt;
&lt;/a&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;So I purchased 1Tb of storage for Google drive for USD 9.99/month, and to my surprise it was without taxes, so I ended up paying USD 12.49/month (~ DKK 85/month), but it is much faster than Dropbox.&lt;/p&gt;

&lt;p&gt;But as duplicity tried to upload the very last file, it encountered an error:&lt;/p&gt;

&lt;pre&gt;Attempt 1 failed. OverflowError: length too large
Attempt 2 failed. OverflowError: length too large
Attempt 3 failed. OverflowError: length too large
Attempt 4 failed. OverflowError: length too large
Giving up after 5 attempts. OverflowError: length too large&lt;/pre&gt;

&lt;p&gt;It looks like it is caused by &lt;a href=&#34;https://github.com/googledrive/PyDrive/issues/27&#34;&gt;this bug&lt;/a&gt; in PyDrive. I could see duplicity using more than 70% memory, and then produce an error.&lt;/p&gt;

&lt;p&gt;Duplicity creates a &amp;quot;sigtar&amp;quot; file of around 3Gb, which is to much for PyDrive to handle.&lt;/p&gt;

&lt;pre&gt;2,9Gb  duplicity-full-signatures.20160425T193345Z.sigtar.gpg
3,1Gb  duplicity-full-signatures.20160425T193345Z.sigtar.part&lt;/pre&gt;

&lt;p&gt;The solution is to split the backup into smaller sets, so instead of backing up the entire home directory up, I have one set for documents, one for pictures and so on.&lt;/p&gt;

&lt;p&gt;So in conclusion I would say that I found almost every thing I looked for, except the cheap part :)&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
